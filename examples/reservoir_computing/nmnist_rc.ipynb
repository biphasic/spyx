{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3a7575",
   "metadata": {},
   "source": [
    "# Training a Reservoir Computer using Cascaded Forward!\n",
    "\n",
    "Train your first SNN in JAX in less than 10 minutes without needing a heavy-duty GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spyx\n",
    "import spyx.nn as snn\n",
    "\n",
    "# JAX imports\n",
    "import os\n",
    "import jax\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".70\"\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# implement our SNN in DeepMind's Haiku\n",
    "import haiku as hk\n",
    "\n",
    "# for surrogate loss training.\n",
    "import optax\n",
    "\n",
    "# rendering tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf2a89",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmnist_dl = spyx.data.NMNIST_loader(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f89744",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmnist_dl.train_step().obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878677e",
   "metadata": {},
   "source": [
    "## SNN\n",
    "\n",
    "Here we define a simple feed-forward SNN using Haiku's RNN features, incorporating our\n",
    "LIF neuron models where activation functions would usually go. Haiku manages all of the state for us, so when we transform the function and get an apply() function we just need to pass the params!\n",
    "\n",
    "Since spiking neurons have a discrete all-or-nothing activation, in order to do gradient descent we'll have to approximate the derivative of the Heaviside function with something smoother. In this case, we use the SuperSpike surrogate gradient from Zenke & Ganguli 2017.\n",
    "Also not that we aren't using bias terms on the linear layers and since the inputs are images, we flatten the data before feeding it to the first layer.\n",
    "\n",
    "Depending on computational constraints, we can use haiku's dynamic unroll to iterate the SNN, or we can use static unroll where the SNN will be unrolled during the JIT compiling process to further increase speed when training on GPU. Note that the static unroll will take longer to compile, but once it runs the iterations per second will be 2x-3x greater than the dynamic unroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reservoir(hk.Module):\n",
    "    def __init__(self, hidden_shape, output_shape, name=\"Resevoir\"):\n",
    "        super().__init__(name)\n",
    "        self.hidden = hidden_shape\n",
    "        self.out = output_shape\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        res = snn.RLIF(self.hidden)(jnp.flatten(x))\n",
    "        decoder_input = jax.lax.stop_gradient(res)\n",
    "        return snn.LI(self.out)(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaFo(hk.Module):\n",
    "    def __init__(self, hidden_shape, output_shape, name=\"Resevoir\"):\n",
    "        super().__init__(name)\n",
    "        self.hidden = hidden_shape\n",
    "        self.out = output_shape\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        res = snn.RLIF(self.hidden)(jnp.flatten(x))\n",
    "        decoder_input = jax.lax.stop_gradient(res)\n",
    "        return decoder_input, snn.LI(self.out)(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeNet(hk.Module):\n",
    "    def __init__(self, num_blocks, hidden_shape, output_shape, name=\"Resevoir\"):\n",
    "        super().__init__(name)\n",
    "        self.hidden = hidden_shape\n",
    "        self.out = output_shape\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        decoder_population = jnp.zeros([self.out])\n",
    "        res, decoded = CaFo(self.hidden, self.out)(x)\n",
    "        decoder_population += decoded\n",
    "        \n",
    "        for i in range(1, self.num_blocks):\n",
    "            res, decoded = CaFo(self.hidden, self.out)(res)\n",
    "            decoder_population += decoded\n",
    "        \n",
    "        return snn.LI(self.out)(decoder_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c01de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_reservoir1(x):\n",
    "    core = hk.DeepRNN([\n",
    "        hk.Linear(64, with_bias=False),\n",
    "        snn.RLIF(64),\n",
    "        jax.lax.stop_gradient,\n",
    "        hk.Linear(20),\n",
    "        snn.LI(20)\n",
    "    ])\n",
    "    spikes, V = hk.dynamic_unroll(core, x.astype(jnp.float16), core.initial_state(x.shape[0]), return_all_states=True, time_major=False)\n",
    "    return spikes, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d200992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_reservoir2(x):\n",
    "    core = hk.DeepRNN([\n",
    "        hk.Linear(64, with_bias=False),\n",
    "        Reservoir(64, 10)\n",
    "    ])\n",
    "    spikes, V = hk.dynamic_unroll(core, x.astype(jnp.float16), core.initial_state(x.shape[0]), return_all_states=True, time_major=False)\n",
    "    return spikes, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(unroll_reservoir))\n",
    "params = SNN.init(rng=key, x=nmnist_dl.train_step().obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711ce25",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We define a training loop below.\n",
    "\n",
    "We use the Lion optimizer from Optax, which is a more efficient competitor to the popular Adam. The eval steps and updates are JIT'ed to maximize time spent in optimized GPU code and minimize time spent in higher-level python.\n",
    "\n",
    "The use of regularizers in the spiking network will be covered in a seperate tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(SNN, params, dl, epochs=50, test_every=5):\n",
    "    \n",
    "    # create and initialize the optimizer\n",
    "    opt = optax.lion(3e-4)\n",
    "    opt_state = opt.init(params)\n",
    "    grad_params = params\n",
    "        \n",
    "    # define and compile our eval function that computes the loss for our SNN\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.loss.integral_crossentropy(traces, targets)\n",
    "        \n",
    "    # Use JAX to create a function that calculates the loss and the gradient!\n",
    "    surrogate_grad = jax.value_and_grad(net_eval) \n",
    "        \n",
    "    # compile the meat of our training loop for speed\n",
    "    @jax.jit\n",
    "    def step(grad_params, opt_state, events, targets):\n",
    "        # compute loss and gradient\n",
    "        loss, grads = surrogate_grad(grad_params, events, targets)\n",
    "        # generate updates based on the gradients and optimizer\n",
    "        updates, opt_state = opt.update(grads, opt_state, grad_params)\n",
    "        # return the updated parameters\n",
    "        return optax.apply_updates(grad_params, updates), opt_state, loss\n",
    "    \n",
    "    # For validation epochs, do the same as before but compute the\n",
    "    # accuracy, predictions and losses (no gradients needed)\n",
    "    @jax.jit\n",
    "    def eval_step(grad_params, events, targets):\n",
    "        readout = SNN.apply(grad_params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.loss.integral_accuracy(traces, targets)\n",
    "        loss = spyx.loss.integral_crossentropy(traces, targets)\n",
    "        return acc, pred, loss\n",
    "        \n",
    "    # Here's the start of our training loop!\n",
    "    for gen in range(epochs):\n",
    "        # make a progress bar with tqdm so things look official\n",
    "        pbar = tqdm([*range(dl.train_len//dl.batch_size)])\n",
    "        pbar.set_description(\"Epoch #{}\".format(gen))\n",
    "        # reset our training data loader so we're at the beginning of the train set\n",
    "        dl.train_reset()\n",
    "        for _ in pbar:\n",
    "            # fetch the batch and the labels\n",
    "            events, targets = dl.train_step() \n",
    "            # compute new params and loss\n",
    "            grad_params, opt_state, loss = step(grad_params, opt_state, events, targets)\n",
    "            #update progress bar\n",
    "            pbar.set_postfix(Loss=loss)\n",
    "            \n",
    "        # after a number of epochs, check performance on validation set\n",
    "        if gen % test_every == test_every-1:\n",
    "            # reset validation iterator\n",
    "            dl.val_reset()\n",
    "            \n",
    "            # containers for SNN results. Can return these if desired.\n",
    "            accs = []\n",
    "            preds = []\n",
    "            losses = []\n",
    "            \n",
    "            # progress bars!\n",
    "            pbar = tqdm([*range(dl.val_len//dl.batch_size)])\n",
    "            pbar.set_description(\"Validating\")\n",
    "            for _ in pbar:\n",
    "                # get validation batch\n",
    "                events, targets = dl.val_step()\n",
    "                # get perfomance on validation batch\n",
    "                acc, pred, loss = eval_step(grad_params, events, targets)\n",
    "                # save accuracy, prediction, loss\n",
    "                accs.append(acc)\n",
    "                preds.append(pred)\n",
    "                losses.append(loss)\n",
    "                # update progress bar, showing running loss and accuracy\n",
    "                pbar.set_postfix(Loss=np.mean(losses), Accuracy=np.mean(accs))\n",
    "                \n",
    "    # return our final, optimized network.       \n",
    "    return grad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gd(SNN, params, dl):\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.loss.integral_crossentropy(traces, targets)\n",
    "    \n",
    "    @jax.jit\n",
    "    def eval_step(grad_params, events, targets):\n",
    "        readout = SNN.apply(grad_params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.loss.integral_accuracy(traces, targets)\n",
    "        loss = spyx.loss.integral_crossentropy(traces, targets)\n",
    "        return acc, pred, loss\n",
    "    \n",
    "    dl.test_reset()\n",
    "    accs = []\n",
    "    preds = []\n",
    "    losses = []\n",
    "    pbar = tqdm([*range(dl.test_len//dl.batch_size)])\n",
    "    pbar.set_description(\"Validating\")\n",
    "    for _ in pbar:\n",
    "        events, targets = dl.test_step()\n",
    "        \n",
    "        acc, pred, loss = eval_step(grad_params, events, targets)\n",
    "        \n",
    "        accs.append(acc)\n",
    "        preds.append(pred)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        pbar.set_postfix(Loss=np.mean(losses), Accuracy=np.mean(accs))\n",
    "    \n",
    "    return accs, preds, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1669fb3",
   "metadata": {},
   "source": [
    "## Training Time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1565db",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_params = gd(SNN, params, nmnist_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b79e4",
   "metadata": {},
   "source": [
    "## Evaluation Time\n",
    "\n",
    "Now we'll run the network on the test set and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9716b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, preds, losses = test_gd(SNN, params, nmnist_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc40d37",
   "metadata": {},
   "source": [
    "Not bad! Now we can investigate the network's predictions using a confusion matrix or other techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
